{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maimuna-noshin/Solar_Prediction/blob/main/solarprediction_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "vQ2XqW7kluLE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pickle\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint as sp_randint\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint as sp_randint\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, BatchNormalization, Flatten\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.fftpack import fft\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/updated_dataset.csv\")"
      ],
      "metadata": {
        "id": "b2EaJDk9l9JD"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to datetime\n",
        "df['date'] = pd.to_datetime(df[['Year', 'Month', 'Day', 'Hour']])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8eWPZASsl9Mm"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting features (X) and target variables (Y)\n",
        "features = ['Hour', 'Temperature_A','Pressure_A', 'Day','Month', 'Surface Albedo_A', 'Cloud Type_A']\n",
        "targets = ['DNI_A']  # Target variables for all three locations"
      ],
      "metadata": {
        "id": "jrLXHLggl9TK"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = df[features].values\n",
        "y = df[targets].values"
      ],
      "metadata": {
        "id": "qSIvZNBO2RrP"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep timestamps before scaling\n",
        "timestamps = df[['Month', 'Day', 'Hour']]"
      ],
      "metadata": {
        "id": "QK_c9pUC2R1G"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler(feature_range=(0, 1000))\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y)\n",
        "# Save the feature scaler\n",
        "with open(\"scaler_X.pkl\", \"wb\") as f:\n",
        "    pickle.dump(scaler_X, f)\n",
        "\n",
        "# Save the target scaler\n",
        "with open(\"scaler_y.pkl\", \"wb\") as f:\n",
        "    pickle.dump(scaler_y, f)"
      ],
      "metadata": {
        "id": "hQ2Kq3h82R42"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hours_tensor = tf.convert_to_tensor(df['Hour'].values, dtype=tf.float32)\n"
      ],
      "metadata": {
        "id": "PQecOZDwu71w"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test, timestamps_train, timestamps_test = train_test_split(X_scaled, y_scaled,timestamps, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "EZ3DTavB2R8l"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss_fixed(y_true, y_pred):\n",
        "    batch_size = tf.shape(y_true)[0]\n",
        "    batch_hours = tf.gather(hours_tensor, tf.range(batch_size))  # Use hours_tensor instead of hours\n",
        "\n",
        "    nighttime_mask = tf.logical_or(tf.less_equal(batch_hours, 8), tf.greater_equal(batch_hours, 18))\n",
        "    nighttime_mask = tf.cast(nighttime_mask, tf.float32)\n",
        "\n",
        "    mse_loss = tf.keras.losses.MSE(y_true, y_pred)\n",
        "    penalty_factor = 50.0\n",
        "    weighted_loss = mse_loss * (1 + (penalty_factor - 1) * nighttime_mask)\n",
        "\n",
        "    return tf.reduce_mean(weighted_loss)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TXzT_iAmLeVi"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(256, activation='relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(len(targets), activation='linear')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztt3RdnS2g_8",
        "outputId": "fd4cb845-f7d1-4b95-cb47-f17ccd00d050"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=custom_loss_fixed, metrics=['mae'])\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "SmmxLfre2R_7"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbCo6VxP2orT",
        "outputId": "c5059f2a-bbcc-41e9-8596-c40dca83b8d0"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 3353709.0000 - mae: 194.1918 - val_loss: 1517535.5000 - val_mae: 115.1331\n",
            "Epoch 2/10\n",
            "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 1589843.2500 - mae: 118.3901 - val_loss: 1370910.8750 - val_mae: 107.5927\n",
            "Epoch 3/10\n",
            "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1456403.0000 - mae: 107.8436 - val_loss: 1239561.5000 - val_mae: 93.0351\n",
            "Epoch 4/10\n",
            "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1333733.3750 - mae: 98.0960 - val_loss: 1210874.5000 - val_mae: 91.3765\n",
            "Epoch 5/10\n",
            "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1303910.2500 - mae: 97.0862 - val_loss: 1176177.3750 - val_mae: 90.3489\n",
            "Epoch 6/10\n",
            "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1249273.0000 - mae: 94.4662 - val_loss: 1182044.1250 - val_mae: 92.1190\n",
            "Epoch 7/10\n",
            "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1172865.3750 - mae: 90.2968 - val_loss: 1151360.5000 - val_mae: 87.3514\n",
            "Epoch 8/10\n",
            "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1241432.1250 - mae: 95.6530 - val_loss: 1162234.1250 - val_mae: 89.2598\n",
            "Epoch 9/10\n",
            "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1164236.5000 - mae: 90.0529 - val_loss: 1137189.8750 - val_mae: 87.4135\n",
            "Epoch 10/10\n",
            "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1197888.8750 - mae: 91.8551 - val_loss: 1206526.6250 - val_mae: 91.9325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict future solar irradiance for all three locations\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOjekJd72ovo",
        "outputId": "1e23cf98-df67-453a-c2f9-3a9d65b64fd0"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 62 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x78c95710d360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions into a DataFrame\n",
        "predictions_df = timestamps_test.copy()\n",
        "\n",
        "# Assign each column separately\n",
        "predictions_df[['DNI_A']] = y_pred"
      ],
      "metadata": {
        "id": "8IdbseJv3CKz"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print first 100 predictions with date and hour\n",
        "print(predictions_df.head(100))\n",
        "predictions_df.to_csv(\"solar_irradiance_predictions.csv\", index=False)  # Saves to a CSV]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98M8JsdV3COJ",
        "outputId": "d979dac3-b597-4b89-f239-bf790246f7d3"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Month  Day  Hour       DNI_A\n",
            "6056      9   10     8  148.175308\n",
            "5556      8   20    12  282.674957\n",
            "5990      9    7    14  321.092865\n",
            "7674     11   16    18  177.015640\n",
            "3319      5   19     7  109.099396\n",
            "...     ...  ...   ...         ...\n",
            "6006      9    8     6    8.087282\n",
            "8063     12    2    23    2.897856\n",
            "14        1    1    14  478.830811\n",
            "5306      8   10     2    1.438244\n",
            "8524     12   22     4    1.004926\n",
            "\n",
            "[100 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"solar_irradiance_predictions.keras\")\n",
        "\n",
        "\n",
        "model = keras.models.load_model(\n",
        "    \"solar_irradiance_predictions.keras\",\n",
        "    custom_objects={\"custom_loss_fixed\": custom_loss_fixed},\n",
        "    safe_mode=False  # Allows deserialization of custom objects\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "puUcFKZDAnOu"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Load the scalers used during training\n",
        "\n",
        "with open(\"scaler_X.pkl\", \"rb\") as f:\n",
        "    scaler_X = pickle.load(f)\n",
        "with open(\"scaler_y.pkl\", \"rb\") as f:\n",
        "    scaler_y = pickle.load(f)"
      ],
      "metadata": {
        "id": "gZGmN3-zAnR0"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate timestamps for the first week of March 2025 (hourly)\n",
        "date_range = pd.date_range(start=\"2025-03-01\", end=\"2025-03-07 23:00:00\", freq=\"H\")\n",
        "future_df = pd.DataFrame({\"datetime\": date_range})"
      ],
      "metadata": {
        "id": "7-2OXJ9dArho",
        "outputId": "3dda5684-cc05-4801-95d6-8692557833c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-153-0e223ba96d09>:2: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  date_range = pd.date_range(start=\"2025-03-01\", end=\"2025-03-07 23:00:00\", freq=\"H\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Feature Engineering\n",
        "future_df[\"day\"] = future_df[\"datetime\"].dt.day\n",
        "future_df[\"month\"] = future_df[\"datetime\"].dt.month\n",
        "future_df[\"hour\"] = future_df[\"datetime\"].dt.hour\n",
        "future_df.drop(columns=[\"datetime\"], inplace=True)"
      ],
      "metadata": {
        "id": "G55MNz34AnVq"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate random values\n",
        "future_df[\"Temperature_A\"] = np.random.uniform(27, 30, size=168)  # Temperature between 15°C to 30°C\n",
        "future_df[\"Pressure_A\"] = np.random.uniform(1000, 1050, size=168)  # Pressure between 950-1050 hPa\n",
        "future_df[\"Surface Albedo_A\"] = np.random.uniform(0.1, 0.5, size=168)  # Albedo between 0.1 - 0.5\n",
        "future_df[\"Cloud Type_A\"] = np.random.randint(0, 4, size=168)  # Cloud types (0-3)\n",
        "\n",
        "# Merge the first week’s random values with `future_df`\n",
        "#future_df = future_df.merge(future_df, on=[\"Month\", \"Day\"], how=\"left\")\n",
        "\n",
        "# Time-based encoding (sin/cos transformation)\n",
        "#future_df[\"hour_sin\"] = np.sin(2 * np.pi * future_df[\"hour\"] / 24)\n",
        "#future_df[\"hour_cos\"] = np.cos(2 * np.pi * future_df[\"hour\"] / 24)\n",
        "\n",
        "# Select the same features used in training\n",
        "#X_future = future_df[['hour_sin', 'hour_cos', 'Temperature_A', 'Pressure_A', 'day', 'Month', 'Surface Albedo_A', 'Cloud Type_A']].values"
      ],
      "metadata": {
        "id": "e7HqObYi70BB"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Select the same features used in training\n",
        "X_future = future_df[['hour', 'Temperature_A','Pressure_A', 'day','month', 'Surface Albedo_A', 'Cloud Type_A']].values\n",
        "#  Scale future data using the trained scaler\n",
        "X_future_scaled = scaler_X.transform(X_future)\n",
        "print(X_future_scaled)\n"
      ],
      "metadata": {
        "id": "_hgX_tASA1fh",
        "outputId": "d8e0679a-5c7b-43b5-9a22-8dc7138f6087",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.63584829 1.25611464 ... 0.18181818 0.96357312 0.22222222]\n",
            " [0.04347826 0.60151976 0.35048912 ... 0.18181818 2.42026451 0.        ]\n",
            " [0.08695652 0.59044547 1.66355813 ... 0.18181818 1.86901089 0.22222222]\n",
            " ...\n",
            " [0.91304348 0.65545849 0.55240681 ... 0.18181818 3.65113786 0.22222222]\n",
            " [0.95652174 0.60447763 0.54152899 ... 0.18181818 0.11682362 0.22222222]\n",
            " [1.         0.62956356 1.10815991 ... 0.18181818 3.22047686 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Make predictions\n",
        "y_future_scaled = model.predict(X_future_scaled)\n",
        "\n",
        "#  Inverse scale predictions\n",
        "y_future = scaler_y.inverse_transform(y_future_scaled)\n",
        "#  Set negative irradiance values to 0\n",
        "y_future[y_future < 15] = 0"
      ],
      "metadata": {
        "id": "j9fFf3iAA19-",
        "outputId": "a7f2e107-8edb-402c-a7a0-ac8df28e7ba9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predictions to DataFrame\n",
        "future_df[['DNI_A']] = y_future\n",
        "\n",
        "#  Save to CSV\n",
        "future_df.to_csv(\"Future_Solar_Predictions_March2025.csv\", index=False)\n",
        "\n",
        "#Display first 10 predictions\n",
        "print(future_df.head(10))"
      ],
      "metadata": {
        "id": "GbNNMPFzBAEQ",
        "outputId": "b744018c-57d2-423c-d1da-8a8a6a538d3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  month  hour  Temperature_A   Pressure_A  Surface Albedo_A  \\\n",
            "0    1      3     0      28.928654  1028.683439          0.188179   \n",
            "1    1      3     1      27.809544  1001.514673          0.261013   \n",
            "2    1      3     2      27.448522  1040.906744          0.233451   \n",
            "3    1      3     3      29.319266  1014.481050          0.345836   \n",
            "4    1      3     4      27.869334  1019.134334          0.230381   \n",
            "5    1      3     5      27.694777  1010.269331          0.481468   \n",
            "6    1      3     6      29.921404  1010.691317          0.328631   \n",
            "7    1      3     7      29.255739  1009.526937          0.294309   \n",
            "8    1      3     8      28.315013  1007.584922          0.274253   \n",
            "9    1      3     9      28.581875  1008.974374          0.413074   \n",
            "\n",
            "   Cloud Type_A       DNI_A  \n",
            "0             2    0.000000  \n",
            "1             0    0.000000  \n",
            "2             2    0.000000  \n",
            "3             0    0.000000  \n",
            "4             3    0.000000  \n",
            "5             0    0.000000  \n",
            "6             0    0.000000  \n",
            "7             2   30.266188  \n",
            "8             3  120.857430  \n",
            "9             3    0.000000  \n"
          ]
        }
      ]
    }
  ]
}